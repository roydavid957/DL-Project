{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "akHYXpkCI8D2"
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from train import run\n",
        "from model import EncoderRNN, LuongAttnDecoderRNN\n",
        "from chat import GreedySearchDecoder, chat"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w98dv-h0rqs_"
      },
      "source": [
        "def run_train(device, encoder_name=\"GRU\", decoder_name=\"GRU\", encoder_direction=2, opt=\"ADAM\", \n",
        "              EPOCH_NUM=50, DROPOUT=0.1, CLIP=10.0, LR=0.001, WD=1e-5, HIDDEN_SIZE=300, \n",
        "              ENCODER_N_LAYERS=2, DECODER_N_LAYERS=2, BATCH_SIZE=64, BIDIRECTION=True, attn_model=\"dot\"):\n",
        "\n",
        "    phase = {\n",
        "        \"train\": {\"pairs\": []},\n",
        "    }\n",
        "\n",
        "    with open(\"formatted_movie_QR_lines_train.txt\", \"r\") as file_obj:\n",
        "        for line in file_obj:\n",
        "            phase[\"train\"][\"pairs\"].append(line.split(\"\\n\")[0].split(\"\\t\"))\n",
        "    with open('voc.pickle', \"rb\") as f:\n",
        "        phase[\"train\"][\"voc\"] = pickle.load(f)\n",
        "\n",
        "    # Shuffle both sets ONCE before the entire training\n",
        "    random.seed(1)  # seed can be any number\n",
        "    random.shuffle(phase[\"train\"][\"pairs\"])\n",
        "\n",
        "    print('Building training set encoder and decoder ...')\n",
        "    # Initialize word embeddings for both encoder and decoder\n",
        "    embedding = nn.Embedding(phase[\"train\"][\"voc\"].num_words, HIDDEN_SIZE).to(device)\n",
        "\n",
        "    # Initialize encoder & decoder models\n",
        "    encoder = EncoderRNN(HIDDEN_SIZE, embedding, ENCODER_N_LAYERS, DROPOUT, gate=encoder_name,\n",
        "                          bidirectional=BIDIRECTION)\n",
        "    decoder = LuongAttnDecoderRNN(attn_model, embedding, HIDDEN_SIZE,\n",
        "                                  phase[\"train\"][\"voc\"].num_words, DECODER_N_LAYERS, DROPOUT, gate=decoder_name)\n",
        "\n",
        "    # Use appropriate device\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    print('Models built and ready to go!')\n",
        "\n",
        "    # Initialize optimizers\n",
        "    print('Building optimizers ...')\n",
        "    if opt == \"ADAM\":\n",
        "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=LR, weight_decay=WD)\n",
        "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=LR, weight_decay=WD)\n",
        "    elif opt == \"SGD\":\n",
        "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=LR)\n",
        "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=LR)\n",
        "    else:\n",
        "        raise ValueError(\"Wrong optimizer type has been given as an argument.\")\n",
        "\n",
        "    # If you have cuda, configure cuda to call\n",
        "    for optimizer in [encoder_optimizer, decoder_optimizer]:\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.cuda()\n",
        "\n",
        "    print(\"Starting Training!\")\n",
        "    run(encoder, decoder, encoder_optimizer, decoder_optimizer, EPOCH_NUM, BATCH_SIZE, CLIP, phase)\n",
        "    \n",
        "    return encoder, decoder, phase[\"train\"][\"voc\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a40zHlaIrqtA",
        "outputId": "db8bd5c0-8b79-43ac-9a9b-91de5373a798"
      },
      "source": [
        "# keep encoder and decoder the same\n",
        "encoder = \"GRU\" # GRU, LSTM, MogLSTM\n",
        "decoder = \"GRU\" # GRU, LSTM, MogLSTM\n",
        "epochs = 10\n",
        "\n",
        "# Get device object\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "encoder, decoder, voc = run_train(encoder_name=encoder, decoder_name=decoder, EPOCH_NUM=epochs, device=device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Building training set encoder and decoder ...\n",
            "Models built and ready to go!\n",
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Number of total pairs used for [TRAIN]: 42532\n",
            "Number of batches used for a [TRAIN] epoch: 664\n",
            "Training for 10 epochs...\n",
            "[TRAIN] Epoch: 1 Loss: 4.80516 BLEU score: 0.08961 33.8 s\n",
            "[TRAIN] Epoch: 2 Loss: 4.48559 BLEU score: 0.09556 33.68 s\n",
            "[TRAIN] Epoch: 3 Loss: 4.22112 BLEU score: 0.11438 33.62 s\n",
            "[TRAIN] Epoch: 4 Loss: 3.81435 BLEU score: 0.14507 33.59 s\n",
            "[TRAIN] Epoch: 5 Loss: 3.31764 BLEU score: 0.19401 33.62 s\n",
            "[TRAIN] Epoch: 6 Loss: 2.82097 BLEU score: 0.25004 33.66 s\n",
            "[TRAIN] Epoch: 7 Loss: 2.37608 BLEU score: 0.30486 33.73 s\n",
            "[TRAIN] Epoch: 8 Loss: 2.0121 BLEU score: 0.3474 33.59 s\n",
            "[TRAIN] Epoch: 9 Loss: 1.72157 BLEU score: 0.38008 33.72 s\n",
            "[TRAIN] Epoch: 10 Loss: 1.49852 BLEU score: 0.40151 33.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_z3t-Uhvb-v"
      },
      "source": [
        "def run_chat(encoder, decoder, device, voc):\n",
        "  encoder = encoder.to(device)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  # Initialize search module\n",
        "  searcher = GreedySearchDecoder(encoder, decoder)\n",
        "  chat(searcher, voc)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE3T6P3c_GUb",
        "outputId": "d35c7842-77f7-4399-f3dc-771e630d0406"
      },
      "source": [
        "run_chat(encoder, decoder, device, voc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> who are you?\n",
            "Bot: i m cynthia . . .\n",
            "> q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSdz9mwHAWqK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}