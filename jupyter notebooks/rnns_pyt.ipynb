{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jh8r9G8p8PWA",
    "outputId": "762e6772-84dc-4af2-9961-eeba0e5e1528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0+cu101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "print(torch.__version__) # this should be at least 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data preparation\n",
    "Altered from the fnn with embeddings solution script\n",
    "\n",
    "added sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTW-YBMa8bzn"
   },
   "outputs": [],
   "source": [
    "# Functions for data preparation\n",
    "\n",
    "def sequence_padding(X_data, max_len):\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \"\"\"\n",
    "\t  Pad and truncate the sequences to a certain\n",
    "\t  \"\"\"\n",
    "    sentence_sequences = pad_sequences(X_data, maxlen=max_len, dtype=\"long\", padding=\"post\", truncating=\"post\", value=0)\n",
    "    return sentence_sequences\n",
    "\n",
    "def get_index(word, word2idx, freeze=False):\n",
    "    \"\"\"\n",
    "    map words to indices\n",
    "    keep special OOV token (_UNK) at position 0\n",
    "    \"\"\"\n",
    "    if word in word2idx:\n",
    "        return word2idx[word]\n",
    "    else:\n",
    "        if not freeze:\n",
    "            word2idx[word]=len(word2idx) #new index\n",
    "            return word2idx[word]\n",
    "        else:\n",
    "            return word2idx[\"_UNK\"]\n",
    "\n",
    "\n",
    "def convert_to_n_hot(X, vocab_size):\n",
    "    out = []\n",
    "    for instance in X:\n",
    "        n_hot = np.zeros(vocab_size)\n",
    "        for w_idx in instance:\n",
    "            n_hot[w_idx] = 1\n",
    "        out.append(n_hot)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def convert_to_one_hot(Y, label2idx, label_size):\n",
    "    out = []\n",
    "    for instance in Y:\n",
    "        one_hot = np.zeros(label_size, dtype=int)\n",
    "        one_hot[label2idx[instance]] = 1\n",
    "        out.append(one_hot)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "# Format required by PyTorch's cross-entropy loss\n",
    "def convert_to_index(Y, label2idx, label_size):\n",
    "    out = []\n",
    "    for instance in Y:\n",
    "        index = label2idx[instance]\n",
    "        out.append(index)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "# Format required by PyTorch's nn.embedding\n",
    "def convert_to_indices(X, seq_len):\n",
    "    out = []\n",
    "    for instance in X:\n",
    "        indices = np.zeros(seq_len, dtype=np.int)\n",
    "        indices[:len(instance)] = instance\n",
    "        indices[len(instance):] = 0\n",
    "        out.append(indices)\n",
    "    return np.array(out)\n",
    "    \n",
    "\n",
    "\n",
    "def load_data(trainfile, devfile, testfile, embed_dim):\n",
    "    ### load data\n",
    "    train_sents, train_y = load_animacy_sentences_and_labels(trainfile)\n",
    "    dev_sents, dev_y = load_animacy_sentences_and_labels(devfile)\n",
    "    test_sents, test_y = load_animacy_sentences_and_labels(testfile)\n",
    "\n",
    "    ### create mapping word to indices\n",
    "    word2idx = {\"_UNK\": 0}  # reserve 0 for OOV\n",
    "\n",
    "    ### convert training etc data to indices\n",
    "    X_train = [[get_index(w,word2idx) for w in x] for x in train_sents]\n",
    "    freeze=True\n",
    "    X_dev = [[get_index(w,word2idx,freeze) for w in x] for x in dev_sents]\n",
    "    X_test = [[get_index(w,word2idx,freeze) for w in x] for x in test_sents]\n",
    "\n",
    "    # Get maximum length\n",
    "    max_len = max([len(sequence) for sequence in X_train])\n",
    "    print(\"max sequence length: {}\".format(max_len))\n",
    "\n",
    "    # Pad the sequences\n",
    "    print(\"padding sequences..\")\n",
    "    X_train = sequence_padding(X_train, max_len)\n",
    "    X_dev = sequence_padding(X_dev, max_len)\n",
    "    X_test = sequence_padding(X_test, max_len)\n",
    "\t\n",
    "    print(\"after word2idx, padding {}\".format(X_train[0]))\n",
    "\n",
    "    vocab_size = len(word2idx)\n",
    "    print(\"#vocabulary size: {}\".format(len(word2idx)))\n",
    "          \n",
    "    X_train = convert_to_indices(X_train, max_len)\n",
    "    X_dev = convert_to_indices(X_dev, max_len)\n",
    "    X_test = convert_to_indices(X_test, max_len)\n",
    "\n",
    "    print(\"after conversion {}\".format(X_train[0]))\n",
    "    \n",
    "    ### convert labels to one-hot\n",
    "    label2idx = {label: i for i, label in enumerate(set(train_y))}\n",
    "    num_labels = len(label2idx.keys())\n",
    "    print(\"#Categories: {}, {}\".format(label2idx.keys(), label2idx.values()))\n",
    "    y_train = convert_to_index(train_y, label2idx, num_labels)\n",
    "    print(train_y[:4], y_train[:4], len(y_train)) # sanity check\n",
    "    y_dev = convert_to_index(dev_y, label2idx, num_labels)\n",
    "    y_test = convert_to_index(test_y, label2idx, num_labels)\n",
    "    print(dev_y[:4], y_dev[:4], len(y_dev)) # sanitiy check\n",
    "    print(test_y[0], y_test[0], len(y_test)) # sanity check\n",
    "\n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test, word2idx, label2idx\n",
    "\n",
    "\n",
    "def load_animacy_sentences_and_labels(datafile):\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    loads the data set\n",
    "    \"\"\"\n",
    "    # load offenseval2019 testset\n",
    "    if 'offenseval' in datafile:\n",
    "      X_data = pd.read_csv(datafile, sep=\"\\t\", header=0, names=['id', 'text'])\n",
    "      # load offenseval2019 labels\n",
    "      data = pd.read_csv('data/offenseval2019/labels-levela.csv', sep=\",\", header=None, names=['id', 'label'])\n",
    "      sentences = [sentence.split() for sentence in X_data['text']]\n",
    "      labels = ['1' if label == 'OFF' else '0' for label in data['label']]\n",
    "    else:\n",
    "      # load hateval2019 data\n",
    "      input = [line.strip().split(\"\\t\")[1:3] for line in open(datafile)]\n",
    "      sentences = [sentence.split() for sentence, label in input[1:]]\n",
    "      labels = [label for sentence, label in input[1:]]\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data\n",
    "loading in train-, dev-, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "6gaNzMw-8erz",
    "outputId": "0688c138-b35b-4aa1-f631-86d2c13abc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data..\n",
      "max sequence length: 63\n",
      "padding sequences..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after word2idx, padding [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "#vocabulary size: 40951\n",
      "after conversion [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "#Categories: dict_keys(['0', '1']), dict_values([0, 1])\n",
      "['1', '1', '1', '0'] [1 1 1 0] 9000\n",
      "['0', '0', '1', '1'] [0 0 1 1] 1000\n",
      "1 1 860\n",
      "#train instances: 9000\n",
      "#dev instances: 1000\n",
      "#test instances: 860\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "## read input data\n",
    "print(\"load data..\")\n",
    "embed_dim = 100\n",
    "\n",
    "# offenseval2019 test\n",
    "test = 'data/offenseval2019/testset-levela.tsv'\n",
    "\n",
    "# hateval2019 data\n",
    "train = 'data/hateval2019/train/public_development_en/train_en.tsv'\n",
    "dev = 'data/hateval2019/train/public_development_en/dev_en.tsv'\n",
    "# test = 'data/hateval2019/test/reference_test_en/en.tsv'\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, word2idx, tag2idx = load_data(train, dev, test, embed_dim)\n",
    "\n",
    "print(\"#train instances: {}\\n#dev instances: {}\\n#test instances: {}\".format(len(X_train),len(X_dev), len(X_test)))\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "assert(len(X_dev)==len(y_dev))\n",
    "\n",
    "vocabulary_size=len(word2idx.keys())\n",
    "num_classes = len(tag2idx)\n",
    "input_size = len(X_train[0])\n",
    "print(input_size) # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (minimal) GRU\n",
    "input -> embedding layer -> GRU -> ReLU -> output layer -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYsyghfe7gZ-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 seq_len, \n",
    "                 embed_dim\n",
    "                 ):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        super(GRU, self).__init__()\n",
    "        ### BEGIN Addition of an embedding layer to the network\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0\n",
    "                                      )\n",
    "        ### END\n",
    "        self.rnn = nn.GRU(\n",
    "                          input_size=embed_dim,  # if taking mean of embeddings\n",
    "                          hidden_size=32,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=False,\n",
    "                          )\n",
    "            \n",
    "        self.fc2 = nn.Linear(32 * 1, 1) # (hidden dimensions*(num_layers*bidirectional(2 if True else 1)), output dimensions)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ### BEGIN Addition of an embedding layer to the network\n",
    "        x = self.embedding(x)  # [batch_size, num_tokens_input, emb_dim]\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        ### END\n",
    "\n",
    "        out, x = self.rnn(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (minimal) LSTM\n",
    "input -> embedding layer -> LSTM -> ReLU -> output layer -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 seq_len, \n",
    "                 embed_dim\n",
    "                 ):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        super(LSTM, self).__init__()\n",
    "        ### BEGIN Addition of an embedding layer to the network\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0\n",
    "                                      )\n",
    "        ### END\n",
    "        self.rnn = nn.LSTM(\n",
    "                          input_size=embed_dim,  # if taking mean of embeddings\n",
    "                          hidden_size=32,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=False,\n",
    "                          )\n",
    "            \n",
    "        self.fc2 = nn.Linear(32 * 1, 1) # (hidden dimensions*(num_layers*bidirectional(2 if True else 1)), output dimensions)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        ### BEGIN Addition of an embedding layer to the network\n",
    "        x = self.embedding(x)  # [batch_size, num_tokens_input, emb_dim]\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        ### END\n",
    "\n",
    "        out, (x, c) = self.rnn(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different accuracy function\n",
    "for binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHDA4mbUuXAT"
   },
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "setting the optimizer, loss function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "NeVsLAxQ8kLa",
    "outputId": "22aefc8f-c4e4-4f76-b73e-99d296a500fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#build model\n",
      "#Model: LSTM(\n",
      "  (embedding): Embedding(40951, 100, padding_idx=0)\n",
      "  (rnn): LSTM(100, 32, batch_first=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import time\n",
    "\n",
    "print(\"#build model\")\n",
    "# model = GRU(input_dim=vocabulary_size, seq_len=input_size, embed_dim=embed_dim)\n",
    "model = LSTM(input_dim=vocabulary_size, seq_len=input_size, embed_dim=embed_dim)\n",
    "print(\"#Model: {}\".format(model))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "setting amount of epochs, batch size\n",
    "\n",
    "shuffling the training data each epoch\n",
    "\n",
    "with validation set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mNNj5Vz_0cCw",
    "outputId": "bc0fc4c1-3498-459b-ecf0-927e3c3288f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training..\n",
      "#Batch size: 100, num batches: 90\n",
      "  End epoch 1. Average loss 0.684. Average acc 0.580. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  End epoch 2. Average loss 0.681. Average acc 0.580. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 3. Average loss 0.681. Average acc 0.580. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 4. Average loss 0.680. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 5. Average loss 0.680. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 6. Average loss 0.680. Average acc 0.580. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 7. Average loss 0.680. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 8. Average loss 0.678. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 9. Average loss 0.677. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 10. Average loss 0.676. Average acc 0.583. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 11. Average loss 0.680. Average acc 0.580. Time 0.53 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 12. Average loss 0.679. Average acc 0.580. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 13. Average loss 0.677. Average acc 0.580. Time 0.52 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.574\n",
      "  End epoch 14. Average loss 0.651. Average acc 0.617. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.641\n",
      "  End epoch 15. Average loss 0.594. Average acc 0.696. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.623\n",
      "  End epoch 16. Average loss 0.474. Average acc 0.801. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.668\n",
      "  End epoch 17. Average loss 0.368. Average acc 0.862. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.659\n",
      "  End epoch 18. Average loss 0.304. Average acc 0.895. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.688\n",
      "  End epoch 19. Average loss 0.246. Average acc 0.923. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.683\n",
      "  End epoch 20. Average loss 0.205. Average acc 0.943. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.694\n",
      "  End epoch 21. Average loss 0.184. Average acc 0.952. Time 0.53 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.691\n",
      "  End epoch 22. Average loss 0.168. Average acc 0.956. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.670\n",
      "  End epoch 23. Average loss 0.153. Average acc 0.961. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.675\n",
      "  End epoch 24. Average loss 0.136. Average acc 0.968. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.683\n",
      "  End epoch 25. Average loss 0.121. Average acc 0.970. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.685\n",
      "  End epoch 26. Average loss 0.120. Average acc 0.971. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.678\n",
      "  End epoch 27. Average loss 0.099. Average acc 0.978. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.674\n",
      "  End epoch 28. Average loss 0.086. Average acc 0.982. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.686\n",
      "  End epoch 29. Average loss 0.080. Average acc 0.984. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.666\n",
      "  End epoch 30. Average loss 0.076. Average acc 0.985. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.670\n",
      "  End epoch 31. Average loss 0.069. Average acc 0.987. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.673\n",
      "  End epoch 32. Average loss 0.067. Average acc 0.987. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.672\n",
      "  End epoch 33. Average loss 0.065. Average acc 0.988. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.673\n",
      "  End epoch 34. Average loss 0.063. Average acc 0.988. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.681\n",
      "  End epoch 35. Average loss 0.061. Average acc 0.989. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.684\n",
      "  End epoch 36. Average loss 0.061. Average acc 0.988. Time 0.47 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.676\n",
      "  End epoch 37. Average loss 0.057. Average acc 0.989. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.676\n",
      "  End epoch 38. Average loss 0.052. Average acc 0.991. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.677\n",
      "  End epoch 39. Average loss 0.053. Average acc 0.990. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.681\n",
      "  End epoch 40. Average loss 0.049. Average acc 0.991. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.677\n",
      "  End epoch 41. Average loss 0.047. Average acc 0.991. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.679\n",
      "  End epoch 42. Average loss 0.052. Average acc 0.989. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.676\n",
      "  End epoch 43. Average loss 0.042. Average acc 0.992. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.679\n",
      "  End epoch 44. Average loss 0.035. Average acc 0.994. Time 0.49 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.681\n",
      "  End epoch 45. Average loss 0.036. Average acc 0.994. Time 0.48 s\n",
      "  Validation\n",
      "    #num batches dev: 10\n",
      "    0.669\n",
      "[!]Early stopping, no increase in last 25 epochs\n",
      "#Highest acc..\n",
      "  Epoch 20. Average loss 0.205. Average acc 0.694\n",
      "    0.694\n",
      "  #Classification report dev..\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       573\n",
      "           1       0.65      0.62      0.63       427\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.68      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#Training..\")\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "size_batch = 100\n",
    "early_stopping = 0\n",
    "num_batches = round(len(X_train) / size_batch)\n",
    "print(\"#Batch size: {}, num batches: {}\".format(size_batch, num_batches))\n",
    "best_epoch_acc = {'epoch': '', 'epoch_loss': '', 'epoch_acc': 0, 'y_pred': []}\n",
    "for epoch in range(num_epochs):\n",
    "    data = list(zip(X_train,y_train))\n",
    "    np.random.shuffle(data)\n",
    "    X_train, y_train = zip(*data)\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_train = 0\n",
    "    y_pred_list = []\n",
    "    for batch in range(num_batches):\n",
    "        batch_begin = batch*size_batch\n",
    "        batch_end = (batch+1)*(size_batch)\n",
    "        X_data = X_train[batch_begin:batch_end]\n",
    "        y_data = y_train[batch_begin:batch_end]\n",
    "        \n",
    "        X_tensor = torch.tensor(X_data, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(y_data, dtype=torch.float32)\n",
    "        X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_tensor.unsqueeze(1))\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc_train += acc.item()\n",
    "        \n",
    "    print(\"  End epoch {}. Average loss {:.3f}. Average acc {:.3f}. Time {:.2f} s\".format(epoch+1, epoch_loss/num_batches, \n",
    "                                                                                          epoch_acc_train/num_batches, time.time()-start))\n",
    "    \n",
    "\n",
    "    print(\"  Validation\")\n",
    "    epoch_acc = 0\n",
    "    num_batches_dev = round(len(X_dev) / size_batch)\n",
    "    print(\"    #num batches dev: {}\".format(num_batches_dev))\n",
    "    with torch.no_grad():\n",
    "      for batch in range(num_batches_dev):\n",
    "          batch_dev_begin = batch*size_batch\n",
    "          batch_dev_end = (batch+1)*(size_batch) if ((batch+1)*(size_batch)) <= len(X_dev) else len(X_dev)\n",
    "\n",
    "          X_data_dev = X_dev[batch_dev_begin:batch_dev_end]\n",
    "          y_data_dev = y_dev[batch_dev_begin:batch_dev_end]\n",
    "          X_tensor_dev = torch.tensor(X_data_dev, dtype=torch.long)\n",
    "          y_tensor_dev = torch.tensor(y_data_dev, dtype=torch.int64)\n",
    "          X_tensor_dev, y_tensor_dev = X_tensor_dev.to(device), y_tensor_dev.to(device)\n",
    "          \n",
    "          y_pred_dev = model(X_tensor_dev)\n",
    "          acc = binary_acc(y_pred_dev, y_tensor_dev.unsqueeze(1))\n",
    "          y_pred_dev = torch.sigmoid(y_pred_dev)\n",
    "          y_pred_dev = torch.round(y_pred_dev)\n",
    "          \n",
    "          epoch_acc += acc.item()\n",
    "          y_pred_list.append(y_pred_dev.cpu().numpy())\n",
    "      \n",
    "      print(\"    {:.3f}\".format(epoch_acc / num_batches_dev))\n",
    "      if (epoch_acc / num_batches_dev) >= best_epoch_acc['epoch_acc']:\n",
    "          torch.save(model, 'model.pt')\n",
    "          best_epoch_acc['epoch'] = epoch+1\n",
    "          best_epoch_acc['epoch_loss'] = epoch_loss / num_batches\n",
    "          best_epoch_acc['epoch_acc'] = epoch_acc / num_batches_dev\n",
    "          best_epoch_acc['y_pred'] = y_pred_list\n",
    "          early_stopping = 0\n",
    "      else:\n",
    "        early_stopping += 1\n",
    "        if early_stopping == 25:\n",
    "          print(\"[!]Early stopping, no increase in last {} epochs\".format(str(early_stopping))) \n",
    "          break\n",
    "\n",
    "print(\"#Highest acc..\")\n",
    "print(\"  Epoch {}. Average loss {:.3f}. Average acc {:.3f}\".format(best_epoch_acc['epoch'], best_epoch_acc['epoch_loss'], best_epoch_acc['epoch_acc']))\n",
    "print(\"    {:.3f}\".format(best_epoch_acc['epoch_acc']))\n",
    "\n",
    "print(\"  #Classification report dev..\")\n",
    "y_preds_dev = [y.squeeze().tolist() for y_batch in best_epoch_acc['y_pred'] for y in y_batch]\n",
    "print(classification_report(y_dev, y_preds_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "setting batch size to 1\n",
    "\n",
    "adding sigmoid and round to get label (RNN output -> 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "XL7MVP8uEZic",
    "outputId": "40874040-3ccc-4295-c105-822d5e2edb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Testing..\n",
      "  #num batches test: 860\n",
      "  #Classification report test..\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       620\n",
      "           1       0.39      0.31      0.34       240\n",
      "\n",
      "    accuracy                           0.67       860\n",
      "   macro avg       0.57      0.56      0.56       860\n",
      "weighted avg       0.65      0.67      0.66       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#Testing..\")\n",
    "# model = torch.load('/content/drive/My Drive/GRUmodel.pt')\n",
    "model = torch.load('/content/drive/My Drive/LSTMmodel.pt')\n",
    "model.eval()\n",
    "size_batch = 1\n",
    "num_batches_test = len(X_test) // size_batch\n",
    "print(\"  #num batches test: {}\".format(num_batches_test))\n",
    "y_pred_list_test = []\n",
    "with torch.no_grad():\n",
    "  for batch in range(num_batches_test):\n",
    "      batch_test_begin = batch*size_batch\n",
    "      batch_test_end = (batch+1)*(size_batch)\n",
    "      X_data_test = X_test[batch_test_begin:batch_test_end]\n",
    "      y_data_test = y_test[batch_test_begin:batch_test_end]\n",
    "      X_tensor_test = torch.tensor(X_data_test, dtype=torch.int64)\n",
    "      y_tensor_test = torch.tensor(y_data_test, dtype=torch.int64)\n",
    "      X_tensor_test, y_tensor_test = X_tensor_test.to(device), y_tensor_test.to(device)\n",
    "      \n",
    "      y_pred_test = model(X_tensor_test)\n",
    "      y_pred_test = torch.sigmoid(y_pred_test)\n",
    "      y_pred_test = torch.round(y_pred_test)\n",
    "      \n",
    "      y_pred_list_test.append(y_pred_test.cpu().numpy())\n",
    "\n",
    "print(\"  #Classification report test..\")\n",
    "y_preds_test = [y.squeeze().tolist() for y_batch in y_pred_list_test for y in y_batch]\n",
    "print(classification_report(y_test, y_preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oLlMvHI2ZlG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
